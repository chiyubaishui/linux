https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/free.html
https://riboseyim.github.io/2017/10/24/Linux-Perf-Picture/
# 总核数 = 物理CPU个数 X 每颗物理CPU的核数 
# 总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数
# 查看物理CPU个数
cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l
# 查看每个物理CPU中core的个数(即核数)
cat /proc/cpuinfo| grep "cpu cores"| uniq
# 查看逻辑CPU的个数
cat /proc/cpuinfo| grep "processor"| wc -l
grep 'model name' /proc/cpuinfo | wc -l

stress工具使用：
命令：taskset –c 逻辑CPU索引号 stress --timeout 持续时间 --cpu 1
参数说明：
-c 服务器逻辑CPU的索引号
--timeout 对CPU施加压力的持续时间
--cpu stress进程数，若值为2，每个stress进程所有消耗的逻辑CPU资源平分
如：对索引为0的逻辑cpu施加压力, 持续30秒
taskset -c 0 stress --timeout 30 --cpu 1

系统响应变慢，首先得定位大致的问题出在哪里，是IO瓶颈、CPU瓶颈、内存瓶颈还是程序导致的系统问题；
3.1 使用top工具能够比较全面的查看我们关注的点:
首先要观察平均负载（load average），
简单来说，平均负载是指单位时间内，系统处于运行状态和不可中断状态的平均进程数，也就是平均活跃进程数，它和 CPU 使用率并没有直接关系。这里我先解释下，
可运行状态和不可中断状态这俩词儿。
所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。
不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps命令中看到
的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。
比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态。
如果此时的进程被打断了，就容易出现磁盘数据与进程数据不一致的问题。
所以，不可中断状态实际上是系统对进程和硬件设备的一种保护机制.
当平均负载比 CPU 个数还大的时候，系统已经出现了过载。三个不同时间间隔的平均值，其实给我们提供了，分析系统负载趋势的数据来源，让我们能更全面、更立体地理解目前的负载状况。
我们还是要回到平均负载的含义上来，平均负载是指单位时间内，处于可运行状态和不可中断状态的进程数。所以，它不仅包括了正在使用 CPU 的进程，还包括等待 CPU 和等待 I/O 的进程。而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。比如：CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的。I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高。大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。

进入交互模式后:
	输入M，进程列表按内存使用大小降序排序，便于我们观察最大内存使用者使用有问题（检测内存泄漏问题）;
	输入P，进程列表按CPU使用大小降序排序，便于我们观察最耗CPU资源的使用者是否有问题；
top第三行显示当前系统的，其中有两个值很关键:
	%id：空闲CPU时间百分比，如果这个值过低，表明系统CPU存在瓶颈；
	%wa：%wa正是磁盘I/O占用CPU百分比的时间片，如果这个值过高，表明IO存在瓶颈；

yum install sysstat	
3.2 分析CPU瓶颈
# -P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据
$ mpstat -P ALL 5
Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)
13:30:06     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
13:30:11     all   50.05    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.95
13:30:11       0    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
13:30:11       1  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00
# 间隔 5 秒后输出一组数据
$ pidstat -u 5 1
13:37:07      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
13:37:12        0      2962  100.00    0.00    0.00    0.00  100.00     1  stress
	
	
	
	
	
3.3. 分析IO瓶颈
测试IO写
dd bs=1M count=20000 if=/dev/zero of=test.dd conv=fdatasync   dd命令测试是IO的顺序写和读方式
#stress  -io 100 -t 100  -i 产生n个进程 每个进程反复调用sync()，sync()用于将内存上的内容写到硬盘上
如果IO存在性能瓶颈，top工具中的%wa会偏高；
进一步分析使用iostat工具:
iostat -xdm 1
-d：单独输出Device结果，不包括cpu结果
-k/-m：输出结果以kB/mB为单位，而不是以扇区数为单位
-x:输出更详细的io设备统计信息
interval/count：每次输出间隔时间，count表示输出次数，不带count表示循环输出
%util         代表磁盘繁忙程度。%util    处理 I/O 请求所占用的时间的百分比，即设备利用率。I/O请求期间CPU时间的百分比（即设备的带宽利用率）。100% 表示磁盘繁忙, 0%表示磁盘空闲。但是注意,磁盘繁忙不代表磁盘(带宽)利用率高  
如果%iowait的值过高，表示硬盘存在I/O瓶颈，%idle值高，表示CPU较空闲，如果%idle值高但系统响应慢时，有可能是CPU等待分配内存，此时应加大内存容量。%idle值如果持续低于10，那么系统的CPU处理能力相对较低，表明系统中最需要解决的资源是CPU。
如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明I/O 队列太长，io响应太慢，则需要进行必要优化。如果avgqu-sz比较大，也表示有当量io在等待。


查看IO的进程
如果没安装，那么 yum install iotop 就可以安装上了。
使用方法为 iotop 或者 iotop -o
-o, --only #显示进程或者线程实际上正在做的I/O，而不是全部的，可以随时切换按o

查找哪个文件引起的I/Owait
lsof 命令可以展示一个进程打开的所有文件，或者打开一个文件的所有进程。从这个列表中，我们可以找到具体是什么文件被写入，根据文件的大小和/proc中io文件的具体数据
我们可以使用-p <pid>的方式来减少输出，pid是具体的进程 
lsof  filename 显示打开指定文件的所有进程
lsof -i 用以显示符合条件的进程情况

3.4  监视内存使用情况
free 查询可用内存
vmstat是Virtual Meomory Statistics（虚拟内存统计）的缩写，可实时动态监视操作系统的虚拟内存、进程、CPU活动。
vmstat 5 5
procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----
r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
6  0      0 27900472 204216 28188356    0    0     0     9    1    2 11 14 75  0  0
9  0      0 27900380 204228 28188360    0    0     0    13 33312 126221 22 20 58  0  0
2  0      0 27900340 204240 28188364    0    0     0    10 32755 125566 22 20 58  0  0
Procs（进程）:
r: 运行队列中进程数量
b: 等待IO的进程数量
Memory（内存）:
swpd: 使用虚拟内存大小
free: 可用内存大小
buff: 用作缓冲的内存大小
cache: 用作缓存的内存大小
Swap:
si: 每秒从交换区写到内存的大小
so: 每秒写入交换区的内存大小
IO：（现在的Linux版本块的大小为1024bytes）
bi: 每秒读取的块数
bo: 每秒写入的块数
system：
in: 每秒中断数，包括时钟中断
cs: 每秒上下文切换数
CPU（以百分比表示）
us: 用户进程执行时间(user time)
sy: 系统进程执行时间(system time)
id: 空闲时间(包括IO等待时间)
wa: 等待IO时间

3.5 进程级
pidstat
# pidstat -u -r -d -t 1        
# -u 默认的参数，显示各个进程的cpu使用统计
# -r 缺页及内存信息
# -d IO 信息
# -t 以线程为统计单位
# -w：显示每个进程的上下文切换情况
# -p 针对特定进程统计
# 1  1 秒统计一次
# pidstat -u
Linux 3.10.0-862.el7.x86_64 (m51) 	11/26/2018 	_x86_64_	(1 CPU)

09:15:25 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command
09:15:25 PM     0         1    0.00    0.05    0.00    0.05     0  systemd
09:15:25 PM     0         2    0.00    0.00    0.00    0.00     0  kthreadd
09:15:25 PM     0         3    0.00    0.02    0.00    0.02     0  ksoftirqd/0
09:15:25 PM     0         9    0.00    0.02    0.00    0.02     0  rcu_sched
09:15:25 PM     0        11    0.00    0.00    0.00    0.00     0  watchdog/0
09:15:25 PM     0        29    0.00    0.00    0.00    0.00     0  khugepaged
09:15:25 PM     0        43    0.00    0.05    0.00    0.05     0  kworker/0:2
09:15:25 PM     0       276    0.00    0.00    0.00    0.00     0  scsi_eh_1
09:15:25 PM     0       279    0.00    0.00    0.00    0.00     0  scsi_eh_2
09:15:25 PM     0       397    0.00    0.17    0.00    0.17     0  kworker/u256:27
09:15:25 PM     0       471    0.00    0.01    0.00    0.01     0  jbd2/dm-0-8
09:15:25 PM     0       541    0.00    0.00    0.00    0.00     0  systemd-journal
09:15:25 PM     0       573    0.02    0.01    0.00    0.03     0  systemd-udevd
09:15:25 PM   999       706    0.00    0.00    0.00    0.00     0  polkitd
09:15:25 PM     0       708    0.00    0.00    0.00    0.00     0  systemd-logind
09:15:25 PM    81       711    0.00    0.00    0.00    0.00     0  dbus-daemon
09:15:25 PM     0       717    0.00    0.00    0.00    0.01     0  NetworkManager
09:15:25 PM     0       923    0.00    0.01    0.00    0.01     0  rsyslogd
09:15:25 PM     0       927    0.00    0.01    0.00    0.02     0  tuned
09:15:25 PM     0       994    0.00    0.04    0.00    0.04     0  sshd
09:15:25 PM     0      1015    0.00    0.00    0.00    0.00     0  master
09:15:25 PM    89      1016    0.00    0.00    0.00    0.00     0  pickup
09:15:25 PM    89      1017    0.00    0.00    0.00    0.00     0  qmgr
09:15:25 PM     0      1018    0.00    0.00    0.00    0.00     0  bash
09:15:25 PM     0     12704    0.00    0.00    0.00    0.00     0  pidstat










